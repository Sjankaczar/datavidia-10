{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Rank-Based Probabilistic Labeling\n",
                "### Peningkatan Robustness Prediksi Kualitas Udara terhadap Pergeseran Distribusi Data\n",
                "\n",
                "**Tim: backburner**\n",
                "\n",
                "Notebook ini berisi implementasi metodologi **Rank-Based Probabilistic Labeling**, sebuah pendekatan forecasting kualitas udara (ISPU) di Jakarta yang menggunakan kombinasi **CatBoost**, **Optuna Hyperparameter Tuning**, dan **Meteorological Physics (PBL Decay)**.\n",
                "\n",
                "Metodologi ini dirancang untuk mencapai skor **F1 Macro** yang tinggi dengan menjaga stabilitas prediksi melalui kalibrasi lintas tahun (2024 vs 2017).\n",
                "\n",
                "### Fitur Utama Metodologi:\n",
                "1. **Strictly Single-Seed (43)**: Menjamin determinisme penuh dan kemudahan audit.\n",
                "2. **Magic Transition Physics**: Menangani transisi mulus dari data jangka pendek (*persistence*) ke pola musiman jangka panjang (*climatology*).\n",
                "3. **Dual-Proxy Calibration**: Menggunakan bobot 70/30 antara tahun kotor dan tahun bersih untuk menghasilkan *threshold* yang universal.\n",
                "4. **Threshold Ranking**: Mengatur distribusi label akhir berdasarkan peringkat probabilitas, bukan sekadar nilai klasifikasi absolut."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Konfigurasi Lingkungan & Inisialisasi\n",
                "Kami menetapkan seed tetap dan mematikan peringatan untuk kejelasan output. Stasiun yang diobservasi adalah DKI1 hingga DKI5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "import optuna\n",
                "from optuna.samplers import TPESampler\n",
                "from catboost import CatBoostClassifier\n",
                "from sklearn.metrics import f1_score\n",
                "import random\n",
                "import os\n",
                "\n",
                "def set_seed(seed=43):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "\n",
                "set_seed(43)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "STATIONS = ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
                "LAG_DAYS = [1, 2, 7, 30]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pemrosesan Data Awal (Data Preprocessing)\n",
                "Fungsi `prep` memuat data ISPU dan Weather untuk 5 stasiun, merapikan format tanggal, dan mengintegrasikannya dalam satu DataFrame pelatihan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def is_hol(d): \n",
                "    if d.dayofweek >= 5: \n",
                "        return 1\n",
                "    if (d.month == 1 and d.day == 1 or \n",
                "        d.month == 5 and d.day == 1 or \n",
                "        d.month == 8 and d.day == 17): \n",
                "        return 1\n",
                "    return 0\n",
                "\n",
                "def prep():\n",
                "    all_i, all_w = [], []\n",
                "    for st in STATIONS:\n",
                "        # Load ISPU Data\n",
                "        ispu_path = f\"ispu new/ispu_{st.lower()}.csv\"\n",
                "        di = pd.read_csv(ispu_path)\n",
                "        di['stasiun'] = st\n",
                "        di['tanggal'] = pd.to_datetime(di['tanggal'], dayfirst=True, errors='coerce')\n",
                "        \n",
                "        for c in ['pm10', 'pm25', 'so2', 'co']:\n",
                "            if c in di.columns: \n",
                "                di[c] = pd.to_numeric(di[c], errors='coerce')\n",
                "        \n",
                "        all_i.append(di[['tanggal', 'stasiun', 'pm10', 'pm25', 'so2', 'co', 'categori']])\n",
                "        \n",
                "        # Load Weather Data (Including PBL Height)\n",
                "        weather_path = f\"cuaca-harian-pbl/weather_{st.lower()}_2013_2025_pbl.csv\"\n",
                "        dw = pd.read_csv(weather_path)\n",
                "        dw['stasiun'] = st\n",
                "        dw['tanggal'] = pd.to_datetime(dw['date'], errors='coerce').dt.tz_localize(None)\n",
                "        all_w.append(dw)\n",
                "        \n",
                "    i_df = pd.concat(all_i, ignore_index=True).dropna(subset=['tanggal'])\n",
                "    w_df = pd.concat(all_w, ignore_index=True).dropna(subset=['tanggal'])\n",
                "    \n",
                "    w_train = w_df[w_df['tanggal'] < '2025-09-01']\n",
                "    df = pd.merge(i_df, w_train.drop(columns=['date']), on=['tanggal', 'stasiun'], how='inner')\n",
                "    \n",
                "    return df.sort_values(['stasiun', 'tanggal']).reset_index(drop=True), w_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Rekayasa Fitur (Feature Engineering)\n",
                "Bagian krusial di mana kita menghitung fitur Lag untuk menangkap momentum polusi, dan Median Regional untuk menstabilkan anomali sensor lokal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def feat(df):\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Target Encoding (0: BAIK, 1: SEDANG, 2: TIDAK SEHAT)\n",
                "    df['y'] = df['categori'].map({\n",
                "        'BAIK': 0, \n",
                "        'SEDANG': 1, \n",
                "        'TIDAK SEHAT': 2, \n",
                "        'SANGAT TIDAK SEHAT': 2\n",
                "    }).fillna(1).astype(int)\n",
                "    \n",
                "    df['month'] = df['tanggal'].dt.month\n",
                "    df['hol'] = df['tanggal'].apply(is_hol)\n",
                "    \n",
                "    # City Aggregates (Stabilisator Regional)\n",
                "    city = df.groupby('tanggal')[['pm10', 'pm25']].median().reset_index().rename(\n",
                "        columns={'pm10':'pm10_city', 'pm25':'pm25_city'}\n",
                "    )\n",
                "    df = pd.merge(df, city, on='tanggal', how='left')\n",
                "    \n",
                "    for st in STATIONS:\n",
                "        idx = df[df['stasiun'] == st].index\n",
                "        \n",
                "        # Lag Features (Momentum 1, 2, 7, 30 hari)\n",
                "        for c in ['pm10', 'pm25', 'pm10_city']:\n",
                "            if c in df.columns:\n",
                "                for l in LAG_DAYS:\n",
                "                    df.loc[idx, f'{c}_lag{l}'] = df.loc[idx, c].shift(l)\n",
                "                    \n",
                "        for l in LAG_DAYS:\n",
                "            df.loc[idx, f'y_lag{l}'] = df.loc[idx, 'y'].shift(l)\n",
                "            \n",
                "    return df.fillna(-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pelatihan Model CatBoost\n",
                "Melatih model CatBoost tunggal dengan bobot seimbang untuk kelas minoritas agar sensitivitas prediksi meningkat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "raw, w_all = prep()\n",
                "data = feat(raw)\n",
                "\n",
                "# Feature Selection\n",
                "features = ['month', 'hol', 'forecast_temp', 'pbl_max', 'wind_speed_10m_mean']\n",
                "for c in ['pm10', 'pm25', 'pm10_city']:\n",
                "    for l in LAG_DAYS:\n",
                "        if f'{c}_lag{l}' in data.columns:\n",
                "            features.append(f'{c}_lag{l}')\n",
                "            \n",
                "for l in LAG_DAYS:\n",
                "    if f'y_lag{l}' in data.columns:\n",
                "        features.append(f'y_lag{l}')\n",
                "        \n",
                "print(f\"Training model on {len(features)} features...\")\n",
                "train = data[data['tanggal'] < '2024-09-01']\n",
                "model = CatBoostClassifier(\n",
                "    iterations=1200, \n",
                "    depth=8, \n",
                "    verbose=0, \n",
                "    random_seed=42, \n",
                "    auto_class_weights='Balanced', \n",
                "    thread_count=1\n",
                ")\n",
                "model.fit(train[features], train['y'])\n",
                "\n",
                "# Cache Probabilities\n",
                "v24 = data[(data['tanggal'] >= '2024-09-01') & (data['tanggal'] <= '2024-11-30')].copy()\n",
                "v17 = data[(data['tanggal'] >= '2017-09-01') & (data['tanggal'] <= '2017-11-30')].copy()\n",
                "v24['p0'], v24['p1'], v24['p2'] = model.predict_proba(v24[features]).T\n",
                "if not v17.empty: v17['p0'], v17['p1'], v17['p2'] = model.predict_proba(v17[features]).T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optuna Squeeze & Dual-Proxy Calibration\n",
                "Tahap di mana kita mencari 'Magic Numbers' melalui 100 percobaan Optuna untuk menemukan kurva transisi dan ambang batas yang paling stabil di skenario 2024 dan 2017."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    # Hyperparameter Transisi Eksponensial\n",
                "    p_start = trial.suggest_float('p_start', 0.75, 0.95)\n",
                "    d_cutoff = trial.suggest_int('d_cutoff', 20, 60)\n",
                "    d_alpha = trial.suggest_float('d_alpha', 10.0, 40.0)\n",
                "    base_w = trial.suggest_float('base_w', 0.3, 0.7)\n",
                "    \n",
                "    # Hyperparameter Decision Ranking\n",
                "    b_perc = trial.suggest_float('b_perc', 0.01, 0.40)\n",
                "    t_perc = trial.suggest_float('t_perc', 0.01, 0.20)\n",
                "    \n",
                "    def calculate_f1(df):\n",
                "        total_f1 = 0\n",
                "        for st in STATIONS:\n",
                "            sv = df[df['stasiun'] == st]\n",
                "            if sv.empty: continue\n",
                "            n = len(sv)\n",
                "            ib, it = int(n*b_perc), int(n*t_perc)\n",
                "            tb = np.sort(sv['p0'].values)[::-1][ib] if ib < n else 1.0\n",
                "            tt = np.sort(sv['p2'].values)[::-1][it] if it < n else 1.0\n",
                "            p = np.where(sv['p0'].values >= tb, 0, np.where(sv['p2'].values >= tt, 2, 1))\n",
                "            total_f1 += f1_score(sv['y'], p, average='macro')\n",
                "        return total_f1 / 5.0\n",
                "        \n",
                "    s24 = calculate_f1(v24)\n",
                "    s17 = calculate_f1(v17) if not v17.empty else s24\n",
                "    \n",
                "    # Dual-Proxy Blended Score (70/30)\n",
                "    return 0.7 * s24 + 0.3 * s17\n",
                "\n",
                "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=41))\n",
                "study.optimize(objective, n_trials=100)\n",
                "bm = study.best_params\n",
                "print(f\"Optimization complete. Best Proxy Value: {study.best_value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generate Ramalan Masa Depan (Sept-Nov 2025)\n",
                "Menerapkan logika transisi eksponensial untuk meramal kondisi atmosfer Jakarta berdasarkan profil klimatologi dan momentum ISPU terakhir."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "last_d = raw['tanggal'].max()\n",
                "ls_avg = raw[raw['tanggal'] <= last_d].groupby('stasiun')[['pm10', 'pm25']].mean()\n",
                "ls_cat = raw[raw['tanggal'] <= last_d].groupby('stasiun').tail(1).set_index('stasiun')['categori'].map({ \n",
                "    'BAIK': 0, 'SEDANG': 1, 'TIDAK SEHAT': 2, 'SANGAT TIDAK SEHAT': 2 }).fillna(1)\n",
                "\n",
                "wh = w_all[w_all['tanggal'] < '2025-01-01'].copy(); wh['month'] = wh['tanggal'].dt.month\n",
                "cw_a = wh.groupby(['stasiun', 'month'])[['pbl_max', 'wind_speed_10m_mean', 'forecast_temp']].mean().reset_index()\n",
                "\n",
                "ih = raw[raw['tanggal'] < '2025-01-01'].copy(); ih['month'] = ih['tanggal'].dt.month\n",
                "ci_a = ih.groupby(['stasiun', 'month'])[['pm10', 'pm25']].mean().reset_index()\n",
                "ci_b = ih.groupby(['stasiun', 'month'])[['pm10', 'pm25']].median().reset_index()\n",
                "\n",
                "def get_forecast_rows(ci_df):\n",
                "    f_rows = []\n",
                "    for st in STATIONS:\n",
                "        sl_m = ls_avg.loc[st]\n",
                "        for i, d in enumerate(pd.date_range('2025-09-01', '2025-11-30')):\n",
                "            if i < bm['d_cutoff']: w = bm['p_start']\n",
                "            else: w = bm['base_w'] + (bm['p_start'] - bm['base_w']) * np.exp(-(i - bm['d_cutoff']) / bm['d_alpha'])\n",
                "            cr = cw_a[(cw_a['stasiun'] == st) & (cw_a['month'] == d.month)].iloc[0]\n",
                "            ci_r = ci_df[(ci_df['stasiun'] == st) & (ci_df['month'] == d.month)].iloc[0]\n",
                "            row = { 'month': d.month, 'hol': is_hol(d), 'forecast_temp': cr['forecast_temp'], 'pbl_max': cr['pbl_max'], 'wind_speed_10m_mean': cr['wind_speed_10m_mean'], 'stasiun': st, 'tanggal': d }\n",
                "            for c in ['pm10', 'pm25']:\n",
                "                val = w * sl_m[c] + (1 - w) * ci_r[c]\n",
                "                for l in LAG_DAYS: row[f'{c}_lag{l}'] = val\n",
                "            h10_city = w * ls_avg['pm10'].mean() + (1 - w) * ci_df[ci_df['month'] == d.month]['pm10'].mean()\n",
                "            for l in LAG_DAYS: row[f'pm10_city_lag{l}'] = h10_city; row[f'y_lag{l}'] = ls_cat.loc[st]\n",
                "            f_rows.append(row)\n",
                "    return pd.DataFrame(f_rows)\n",
                "\n",
                "f_a = get_forecast_rows(ci_a); f_b = get_forecast_rows(ci_b)\n",
                "pf = (model.predict_proba(f_a[features].fillna(-1)) + model.predict_proba(f_b[features].fillna(-1))) / 2.0\n",
                "f_a['p0'], f_a['p2'] = pf[:, 0], pf[:, 2]\n",
                "f_a['id'] = f_a['tanggal'].dt.strftime('%Y-%m-%d') + '_' + f_a['stasiun']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Mengelompokkan & Mengekspor Submisi\n",
                "Langkah akhir untuk menetapkan label berdasarkan peringkat probabilitas dan menghasilkan file CSV submisi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "final = []\n",
                "for st in STATIONS:\n",
                "    sf = f_a[f_a['stasiun'] == st].copy(); n = len(sf)\n",
                "    ib, it = int(n * bm['b_perc']), int(n * bm['t_perc'])\n",
                "    tb = np.sort(sf['p0'].values)[::-1][ib] if ib < n else 1.0\n",
                "    tt = np.sort(sf['p2'].values)[::-1][it] if it < n else 1.0\n",
                "    p = np.where(sf['p0'].values >= tb, 0, np.where(sf['p2'].values >= tt, 2, 1))\n",
                "    sf['category'] = [['BAIK', 'SEDANG', 'TIDAK SEHAT'][int(x)] for x in p]\n",
                "    final.append(sf)\n",
                "    \n",
                "res = pd.concat(final).sort_values(['tanggal', 'stasiun'])\n",
                "res[['id', 'category']].to_csv('submission_backburner_rank_based.csv', index=False)\n",
                "print(\"Submission successful. Final file: submission_backburner_rank_based.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
